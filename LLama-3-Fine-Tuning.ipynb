{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e38a368-f2b1-4fda-9138-734227530862",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Experiment 1: Fine Tuning LLama 3 on Medical Datasets\n",
    ">\n",
    "![image](https://github.com/Basel-anaya/LoreWeaver/assets/81964452/9ad6eca1-7cc4-4907-a4e6-ae056b8d0b23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19203ea1-ab21-49e7-9900-9c38ba770d2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fine tuning LLana 3 on Medical Dataset\n",
    "- This code is an example of fine-tuning the LLaVA model from the PATHVQA dataset.\n",
    "\n",
    "- The code first imports the required libraries and modules. It then loads the `PATHVQA dataset` using the load_dataset function from the datasets library.\n",
    "\n",
    "- The dataset is preprocessed using the `CLIPProcessor` to extract features from the images and tokenize the questions and answers. The preprocessed data is then converted to the PyTorch format and stored in the dataset variable.\n",
    "\n",
    "- The `PATHVQAFineTuner` class is defined, which handles loading the model and tokenizer, preprocessing the image, fine-tuning the model, and saving the model.\n",
    "\n",
    "- Inside the class, the `load_model_and_tokenizer` method loads the model and tokenizer from the specified directory. The model is wrapped with `DeepSpeed` for distributed training.\n",
    "\n",
    "- The `preprocess_image` method takes an image path and preprocesses it for model input.\n",
    "\n",
    "- The `fine_tune` method fine-tunes the model using the provided dataset and training arguments.\n",
    "\n",
    "- The `save_model` method saves the fine-tuned model and tokenizer.\n",
    "\n",
    "- The `predict_answer` method takes a question and preprocessed image tensor and generates an answer using the fine-tuned model.\n",
    "\n",
    "- At the end of the code, an instance of the `PATHVQAFineTuner` class is created, the model and tokenizer are loaded, and the model is fine-tuned using the provided dataset. Finally, the fine-tuned model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c072d83-1823-452e-882f-a5d377e194e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U datasets\n",
    "%pip install -U transformers\n",
    "%pip install bitsandbytes\n",
    "%pip install -U peft\n",
    "%pip install -U trl\n",
    "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "%pip uninstall -y torch\n",
    "%pip install torch==2.1.2\n",
    "%pip install accelerate==0.30.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57044c1-d9ca-4e27-988a-42d90f6cedc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca184a7-b4dd-4aa8-af06-c8a40ed6434b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20895d0-f148-48a7-9500-0d42fb674d41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/Workspace/Users/baselanaya@gmail.com/')\n",
    "import torch\n",
    "import torchvision\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "from huggingface_hub import upload_folder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from medformer.custom_optimizer import SophiaG\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          deepspeed,\n",
    "                          PreTrainedTokenizerFast, \n",
    "                          logging)\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd5d512-4d37-4863-b6ab-4ed06a5b7d52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Medical Dataset Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285e0cff-4d79-4d78-a82e-e81bf56b05b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a6d37ca-0c52-44ae-b5b7-8734788b9a11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:45: UserWarning: The cache_dir for this dataset is /root/.cache, which is not a persistent path.Therefore, if/when the cluster restarts, the downloaded dataset will be lost.The persistent storage options for this workspace/cluster config are: [DBFS].Please update either `cache_dir` or the environment variable `HF_DATASETS_CACHE`to be under one of the following root directories: ['/dbfs/']\n  warnings.warn(warning_message)\n/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:14: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Initialize a GPT2Tokenizer instance\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the dataset\n",
    "sft_dataset = load_dataset(\"medarc/sft_multimedqa\", split=\"all\")\n",
    "\n",
    "def preprocess_text_example(example, input_text_column, target_text_column, tokenizer):\n",
    "    # Load and preprocess the input text\n",
    "    input_text = example[input_text_column]\n",
    "    \n",
    "    # Tokenize the input text using GPT2Tokenizer\n",
    "    input_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\").squeeze(0)\n",
    "    \n",
    "    # Prepare the target text for generation\n",
    "    target_text = example[target_text_column]\n",
    "    target_ids = tokenizer.encode(target_text, add_special_tokens=False, return_tensors=\"pt\").squeeze(0)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": target_ids\n",
    "    }\n",
    "\n",
    "# Map the preprocess_text_example function over the dataset\n",
    "sft_dataset = sft_dataset.map(\n",
    "    lambda example: preprocess_text_example(\n",
    "        example,\n",
    "        input_text_column=\"prompt\",\n",
    "        target_text_column=\"completion\",\n",
    "        tokenizer=tokenizer\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Set the dataset format for torch\n",
    "sft_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a483fe4-0576-4c61-9c41-9220e532ccdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DeepSpeed Configurations (ZeRO-Stage III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04479aaa-279f-4810-ab8a-3012f3ef0a18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"fp16\": {\n",
    "      \"enabled\": \"auto\",\n",
    "      \"loss_scale\": 0,\n",
    "      \"loss_scale_window\": 1000,\n",
    "      \"initial_scale_power\": 16,\n",
    "      \"hysteresis\": 2,\n",
    "      \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"bf16\": {\n",
    "      \"enabled\": \"auto\"\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"AdamW\",\n",
    "      \"params\": {\n",
    "        \"lr\": \"auto\",\n",
    "        \"betas\": \"auto\",\n",
    "        \"eps\": \"auto\",\n",
    "        \"weight_decay\": \"auto\"\n",
    "      }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "      \"type\": \"WarmupLR\",\n",
    "      \"params\": {\n",
    "        \"warmup_min_lr\": \"auto\",\n",
    "        \"warmup_max_lr\": \"auto\",\n",
    "        \"warmup_num_steps\": \"auto\"\n",
    "      }\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "      \"stage\": 3,\n",
    "      \"offload_optimizer\": {\n",
    "        \"device\": \"cpu\",\n",
    "        \"pin_memory\": True\n",
    "      },\n",
    "      \"offload_param\": {\n",
    "        \"device\": \"cpu\",\n",
    "        \"pin_memory\": True\n",
    "      },\n",
    "      \"overlap_comm\": True,\n",
    "      \"contiguous_gradients\": True,\n",
    "      \"sub_group_size\": 1e9,\n",
    "      \"reduce_bucket_size\": \"auto\",\n",
    "      \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "      \"stage3_param_persistence_threshold\": \"auto\",\n",
    "      \"stage3_max_live_parameters\": 1e9,\n",
    "      \"stage3_max_reuse_distance\": 1e9,\n",
    "      \"gather_16bit_weights_on_model_save\": True\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"steps_per_print\": 1e5,\n",
    "    \"wall_clock_breakdown\": False\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30530818-d46d-462d-b875-e09bf6b4aa73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fine Tuning Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a125983-1cac-4579-8efe-4e90d2dd80d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    def __init__(self, model_name_or_path, output_dir, local_rank=1, deepspeed_config=ds_config, learning_rate=0.001, per_device_train_batch_size=16):\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.output_dir = output_dir\n",
    "        self.local_rank = local_rank\n",
    "        self.dataset = sft_dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = 0.001\n",
    "        self.device = torch.device('cuda', local_rank) if torch.cuda.is_available() and local_rank >= 0 else torch.device('cpu')\n",
    "        self.per_device_train_batch_size = per_device_train_batch_size\n",
    "\n",
    "        # Load tokenizer and model with QLoRA configuration\n",
    "        self.bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=False\n",
    "        )\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "\n",
    "        # Define training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            per_device_train_batch_size=8,\n",
    "            gradient_accumulation_steps=1,\n",
    "            deepspeed=ds_config,\n",
    "            optim='lion_8bit' \n",
    "        )  \n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)\n",
    "\n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name_or_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            quantization_config=self.bnb_config,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "        self.model, self.tokenizer = setup_chat_format(self.model, self.tokenizer)\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "    def fine_tune(\n",
    "            self,\n",
    "            dataset,\n",
    "            output_dir_prefix=\"./checkpoints/\",\n",
    "            num_train_epochs=2\n",
    "            per_device_train_batch_size=16,\n",
    "            gradient_accumulation_steps=4,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.03,\n",
    "            save_steps=25,\n",
    "            logging_steps=25,\n",
    "            max_grad_norm=0.3,\n",
    "            max_seq_length=1024,\n",
    "            device_map=\"auto\",\n",
    "            learning_rate=0.001,\n",
    "            weight_decay=0.001,\n",
    "            max_steps=-1\n",
    "    ):\n",
    "        dataset = sft_dataset\n",
    "        dataset_text_field = \"prompt\"\n",
    "\n",
    "        # Initialize distributed training if enabled\n",
    "        if self.local_rank != -1:\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        optimizer = bnb.optim.Adam8bit(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "\n",
    "        # Define training arguments\n",
    "        training_arguments = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim='lion_8bit',\n",
    "            save_steps=save_steps,\n",
    "            logging_steps=logging_steps,\n",
    "            bf16=True,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            deepspeed=ds_config,\n",
    "            local_rank=self.local_rank\n",
    "        )\n",
    "\n",
    "        # Initialize DoRA configuration\n",
    "        peft_config = LoraConfig(\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.05,\n",
    "                r=16,\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "                target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                                 \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "                use_dora=True\n",
    "        )\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "                model=self.model,\n",
    "                train_dataset=dataset,\n",
    "                peft_config=peft_config,\n",
    "                dataset_text_field=\"prompt\",\n",
    "                max_seq_length=max_seq_length,\n",
    "                tokenizer=self.tokenizer,\n",
    "                args=training_arguments,\n",
    "                optimizers='lion_8bit',\n",
    "        )\n",
    "\n",
    "        # Initialize DeepSpeed\n",
    "        deepspeed_init(\n",
    "            trainer=trainer,\n",
    "            num_training_steps=num_training_steps,\n",
    "            inference=False,\n",
    "            optimizer_cls=training_args.optim,\n",
    "            resume_from_checkpoint=resume_from_checkpoint,\n",
    "            auto_find_batch_size=auto_find_batch_size,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        lora_model = PeftModel.from_pretrained(trainer.model)\n",
    "        lora_model.save_pretrained(f\"{output_dir_prefix}\")\n",
    "\n",
    "        # Save the fine-tuned model\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        # Save fine-tuned model\n",
    "        if self.local_rank in [-1, 0]:\n",
    "            try:\n",
    "                self.model.save_pretrained(self.output_dir)\n",
    "                self.tokenizer.save_pretrained(self.output_dir)\n",
    "                print(\"Model and tokenizer saved successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model and tokenizer: {e}\")\n",
    "\n",
    "        # Wait for all processes to finish\n",
    "        if self.local_rank != -1:\n",
    "            dist.barrier()\n",
    "\n",
    "# Fine-tune the model on all datasets\n",
    "finetuner = FineTuner(model_name_or_path='meta-llama/Meta-Llama-3-8B', output_dir='./checkpoints/finetuned_model', learning_rate=0.001)\n",
    "finetuner.load_model_and_tokenizer()\n",
    "\n",
    "# Fine-tune the model\n",
    "finetuner.fine_tune(\n",
    "   dataset=sft_dataset,\n",
    "   output_dir_prefix=\"./checkpoints/\",\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "finetuner.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26540d2-2bcf-486f-8f23-259a69c8349f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Merging the LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53efae9-1032-467b-ae3d-e8fed1b6a4d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the original LLaMA model\n",
    "original_llama_model = load_pretrained_model(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "# Load the LoRA weights\n",
    "adapter = PeftModel.from_pretrained(original_idefics2_model, \"Reverb/medllama3-8B\")\n",
    "\n",
    "# Combine the adapters using the add_weighted_adapter method\n",
    "combined_model = adapter.merge_and_unload()\n",
    "\n",
    "# Save the final fine-tuned LLaMA model\n",
    "combined_model.save_pretrained(\"./final_model/medllama3-8B\", is_main_process=True)\n",
    "tokenizer.save_pretrained(\"./final_model/medllama3-8B\")\n",
    "\n",
    "# Deploying the final fine-tuned LLaMA model to Hugging Face\n",
    "upload_folder(\n",
    "    folder_path=\"./final_model/medllama3-8B\",\n",
    "    repo_id=\"Reverb/medllama3-8B\",\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload fine-tuned LLaVA model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLama-3-Fine-Tuning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
